{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from cvxpylayers.torch import CvxpyLayer\n",
    "torch.set_default_dtype(torch.double)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy.random import default_rng\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "rng = default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.33, 0.33, 0.34]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "m = 30\n",
    "p = 0.008\n",
    "std = 0.7\n",
    "\n",
    "outlier = 5\n",
    "means = np.array([[2, 2], [-2, -2], [5, -1]])\n",
    "(K, n) = means.shape\n",
    "\n",
    "probs = rng.random(K)\n",
    "probs = probs/probs.sum()\n",
    "probs = [0.33, 0.33, 0.34]\n",
    "print(probs)\n",
    "\n",
    "groups = rng.choice(np.arange(0, K), size = m, p=probs)\n",
    "\n",
    "means = np.array([[2, 2], [-2, -2], [5, -1]])\n",
    "K = len(means)\n",
    "X_train = []\n",
    "X_test = []\n",
    "for pt in range(len(groups)):\n",
    "    dist_type = groups[pt]\n",
    "    X_train.append(means[dist_type] + std*rng.normal(size = n))\n",
    "    X_test.append(means[dist_type] + std*rng.normal(size = n))\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model(m,n):\n",
    "    a = cp.Variable((m,n))\n",
    "    \n",
    "    ws = cp.Parameter(int(m*(m-1)/2), nonneg = True)\n",
    "    X_train = cp.Parameter((m,n))\n",
    "\n",
    "    error = cp.norm(a - X_train, \"fro\")\n",
    "    regularization =  0\n",
    "\n",
    "    for k in range(int(m*(m-1)/2)):\n",
    "        #print(index)\n",
    "        (i,j) = indices[k]\n",
    "        regularization += ws[k]*cp.norm(a[i,:] - a[j,:], 1)\n",
    "\n",
    "    obj = cp.Minimize(error + regularization)\n",
    "    prob = cp.Problem(obj)\n",
    "    \n",
    "    return CvxpyLayer(prob, [ws, X_train], [a])\n",
    "\n",
    "opt_layer = base_model(m,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1\n",
    "m = X_train.shape[0]\n",
    "assignment = np.zeros(int(m*(m-1)/2))\n",
    "k = 0\n",
    "indices = {}\n",
    "for i in range(m):\n",
    "    for j in range(m):\n",
    "        if i < j:\n",
    "            indices[k] = (i,j)\n",
    "            norm = np.linalg.norm(X_train[i,:] - X_train[j,:], ord = 1)\n",
    "            assignment[k] = norm \n",
    "            k = k+1\n",
    "ws_tch, X_train_tch, X_test_tch = map(torch.from_numpy, [np.exp((-1)*gamma*assignment), X_train, X_test])\n",
    "X_train_tch.requires_grad_(True);\n",
    "ws_tch.requires_grad_(True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tch,  = opt_layer(ws_tch, X_train_tch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(X, a_tch):\n",
    "    loss = torch.norm(X - a_tch,p='fro')\n",
    "    return loss\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, fit_lr, loss_fn):\n",
    "        super(Net, self).__init__()\n",
    "        self.pdist = nn.functional.pdist\n",
    "        self.Relu = nn.ReLU()\n",
    "        self.gamma = nn.Parameter(torch.tensor([1.0]))\n",
    "        self.lamb_tch = nn.Parameter(torch.tensor([1.0]))\n",
    "        \n",
    "        self.perf_loss = loss_fn\n",
    "\n",
    "#         self.lamb_tch.requires_grad_(True);\n",
    "#         self.ws_tch.requires_grad_(True);\n",
    "            \n",
    "        self.cvx_lr = fit_lr\n",
    "        \n",
    "    # X represents our data\n",
    "    def forward(self, X):\n",
    "        #calculate pairwise distances\n",
    "        norms = self.pdist(X)\n",
    "        gamma_pos = self.Relu(self.gamma)\n",
    "        lamb_tch_pos = self.Relu(self.lamb_tch)\n",
    "        ws_tch = torch.mul(lamb_tch_pos, torch.exp(torch.mul(norms, (-1.0)*gamma_pos)))\n",
    "        # Use the rectified-linear activation function over x\n",
    "        a_tch = self.cvx_lr(ws_tch, X)\n",
    "        \n",
    "        return a_tch[0]\n",
    "    \n",
    "    #-----------------------------------------------------------------------------------------------\n",
    "    # net_train: Train the e2e neural net\n",
    "    #-----------------------------------------------------------------------------------------------\n",
    "    def net_train(self, train_set, val_set=None, epochs=None, lr=None):\n",
    "        \"\"\"Neural net training module\n",
    "        \n",
    "        Inputs\n",
    "        train_set: SlidingWindow object containing feaatures x, realizations y and performance\n",
    "        realizations y_perf\n",
    "        val_set: SlidingWindow object containing feaatures x, realizations y and performance\n",
    "        realizations y_perf\n",
    "        epochs: Number of training epochs\n",
    "        lr: learning rate\n",
    "        Output\n",
    "        Trained model\n",
    "        (Optional) val_loss: Validation loss\n",
    "        \"\"\"\n",
    "\n",
    "        # Assign number of epochs and learning rate\n",
    "        if epochs is None:\n",
    "            epochs = self.epochs\n",
    "        if lr is None:\n",
    "            lr = self.lr\n",
    "        \n",
    "        # Define the optimizer and its parameters\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        # Number of elements in training set\n",
    "        n_train = len(train_set)\n",
    "\n",
    "        # Train the neural network\n",
    "        for epoch in range(epochs):\n",
    "                \n",
    "            # TRAINING: forward + backward pass\n",
    "            train_loss = 0\n",
    "            optimizer.zero_grad() \n",
    "            for t, (X_prev, X_next) in enumerate(train_set):\n",
    "\n",
    "                # Forward pass: predict and optimize\n",
    "                a_hat = self(X_prev)\n",
    "\n",
    "                # Loss function\n",
    "    \n",
    "                loss = (1/n_train) * self.perf_loss(X_next, a_hat)\n",
    "                print(loss)\n",
    "                # Backward pass: backpropagation\n",
    "                loss.backward()\n",
    "\n",
    "                # Accumulate loss of the fully trained model\n",
    "                train_loss += loss.item()\n",
    "        \n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "\n",
    "#             # Ensure that gamma, delta > 0 after taking a descent step\n",
    "#             for name, param in self.named_parameters():\n",
    "#                 if name=='gamma':\n",
    "#                     param.data.clamp_(0.0001)\n",
    "#                 if name=='delta':\n",
    "#                     param.data.clamp_(0.0001)\n",
    "\n",
    "        # Compute and return the validation loss of the model\n",
    "        if val_set is not None:\n",
    "\n",
    "            # Number of elements in validation set\n",
    "            n_val = len(val_set)\n",
    "\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for t, (X_prev, X_next) in enumerate(val_set):\n",
    "\n",
    "                    # Predict and optimize\n",
    "                    a_hat = self(X_prev)\n",
    "                \n",
    "                    # Loss function\n",
    "                 \n",
    "                    loss = (1/n_val) * self.perf_loss(z_val, y_perf.squeeze())\n",
    "                  \n",
    "                    # Accumulate loss\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            return train_loss, val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(opt_layer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (Relu): ReLU()\n",
       "  (cvx_lr): CvxpyLayer()\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\anaconda3\\envs\\cvxpy\\lib\\site-packages\\diffcp\\cone_program.py:296: UserWarning: Solved/Inaccurate.\n",
      "  warnings.warn(\"Solved/Inaccurate.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.2227, grad_fn=<MulBackward0>)\n",
      "tensor(6.9704, grad_fn=<MulBackward0>)\n",
      "tensor(6.7695, grad_fn=<MulBackward0>)\n",
      "tensor(6.6072, grad_fn=<MulBackward0>)\n",
      "tensor(6.4739, grad_fn=<MulBackward0>)\n",
      "tensor(6.3637, grad_fn=<MulBackward0>)\n",
      "tensor(6.2714, grad_fn=<MulBackward0>)\n",
      "tensor(6.1935, grad_fn=<MulBackward0>)\n",
      "tensor(6.1272, grad_fn=<MulBackward0>)\n",
      "tensor(6.0706, grad_fn=<MulBackward0>)\n",
      "tensor(6.0218, grad_fn=<MulBackward0>)\n",
      "tensor(5.9792, grad_fn=<MulBackward0>)\n",
      "tensor(5.9421, grad_fn=<MulBackward0>)\n",
      "tensor(5.9096, grad_fn=<MulBackward0>)\n",
      "tensor(5.8810, grad_fn=<MulBackward0>)\n",
      "tensor(5.8558, grad_fn=<MulBackward0>)\n",
      "tensor(5.8333, grad_fn=<MulBackward0>)\n",
      "tensor(5.8133, grad_fn=<MulBackward0>)\n",
      "tensor(5.7954, grad_fn=<MulBackward0>)\n",
      "tensor(5.7793, grad_fn=<MulBackward0>)\n",
      "tensor(5.7648, grad_fn=<MulBackward0>)\n",
      "tensor(5.7517, grad_fn=<MulBackward0>)\n",
      "tensor(5.7399, grad_fn=<MulBackward0>)\n",
      "tensor(5.7291, grad_fn=<MulBackward0>)\n",
      "tensor(5.7193, grad_fn=<MulBackward0>)\n",
      "tensor(5.7103, grad_fn=<MulBackward0>)\n",
      "tensor(5.7021, grad_fn=<MulBackward0>)\n",
      "tensor(5.6946, grad_fn=<MulBackward0>)\n",
      "tensor(5.6876, grad_fn=<MulBackward0>)\n",
      "tensor(5.6812, grad_fn=<MulBackward0>)\n",
      "tensor(5.6753, grad_fn=<MulBackward0>)\n",
      "tensor(5.6698, grad_fn=<MulBackward0>)\n",
      "tensor(5.6647, grad_fn=<MulBackward0>)\n",
      "tensor(5.6599, grad_fn=<MulBackward0>)\n",
      "tensor(5.6555, grad_fn=<MulBackward0>)\n",
      "tensor(5.6514, grad_fn=<MulBackward0>)\n",
      "tensor(5.6475, grad_fn=<MulBackward0>)\n",
      "tensor(5.6439, grad_fn=<MulBackward0>)\n",
      "tensor(5.6405, grad_fn=<MulBackward0>)\n",
      "tensor(5.6372, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "net.net_train([(X_train_tch, X_test_tch)] , val_set=None, epochs=40, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\anaconda3\\envs\\cvxpy\\lib\\site-packages\\diffcp\\cone_program.py:296: UserWarning: Solved/Inaccurate.\n",
      "  warnings.warn(\"Solved/Inaccurate.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1385be2ca90>]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for i in range(m):\n",
    "#     points = [a_values[k][i] for k in range(len(lamb_vals))]\n",
    "#     x = [point[0] for point in points]\n",
    "#     y = [point[1] for point in points]\n",
    "#     plt.plot(x,y, color =  'grey')\n",
    "plt.plot(X_train[:,0], X_train[:,1], 'o', color = 'black')\n",
    "plt.plot(X_train[:,0].mean(), X_train[:,1].mean(), 'o', markersize=10)\n",
    "plt.plot(X_test[:,0], X_test[:,1], 'o', color = 'blue')\n",
    "pred = net(X_train_tch)\n",
    "plt.plot(pred[:,0].detach().numpy(), pred[:,1].detach().numpy(), 'o', markersize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
